---
#name: amba-analysis-stream

#networks:
#  events_kafka:
#    name: events_kafka
version: "3"

services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    #    restart: unless-stopped
    hostname: zookeeper
#    ports:
#      - "2181:2181"
    networks:
      - backend


  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
#    ports:
#      - "9092:9092"
    hostname: kafka
    links:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_CREATE_TOPICS: "${KAFKA_CREATE_TOPICS}"
      KAFKA_ADVERTISED_HOST_NAME: "${KAFKA_ADVERTISED_HOST_NAME}"
      KAFKA_ZOOKEEPER_CONNECT: "${KAFKA_ZOOKEEPER_CONNECT}"
      KAFKA_ADVERTISED_PORT: "${KAFKA_ADVERTISED_PORT}"
      KAFKA_ADVERTISED_LISTENERS: "${KAFKA_ADVERTISED_LISTENERS_PREFIX}${KAFKA_BOOTRSTRAP_SERVER}"
    networks:
      - backend


  mongo_db:
    image: mongo:5.0.2
    container_name: mongo-db
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
    networks:
      - backend

#    volumes:
#        - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
# create this file temporary during github workflow

  connector_twitter:
    image: ghcr.io/ambalytics/amba-connector-twitter/amba-connector-twitter:latest
    container_name: twitter-connector
    depends_on:
      - kafka
    environment:
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
      BERARER_TOKEN: ${TWITTER_BEARER_TOKEN}
    networks:
      - backend

  perculator:
    image: ghcr.io/ambalytics/amba-analysis-worker-perculator/amba-analysis-worker-perculator:latest
    container_name: twitter-perculator
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 100M
    depends_on:
      - connector_twitter
    environment:
      #      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      #      POSTGRES_USER: ${POSTGRES_USER}
      #      POSTGRES_DB: ${POSTGRES_DB}
      #      POSTGRES_HOST: ${POSTGRES_HOST}
      #      POSTGRES_PORT: ${POSTGRES_PORT}
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend

  worker_twitter:
    image: ghcr.io/ambalytics/amba-analysis-worker-discussion/amba-analysis-worker-twitter:latest
    container_name: twitter-worker
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 100M
    depends_on:
      - connector_twitter
    environment:
      #      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      #      POSTGRES_USER: ${POSTGRES_USER}
      #      POSTGRES_DB: ${POSTGRES_DB}
      #      POSTGRES_HOST: ${POSTGRES_HOST}
      #      POSTGRES_PORT: ${POSTGRES_PORT}
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend

  connector_mongodb:
    image: ghcr.io/ambalytics/amba-connector-mongodb/amba-connector-mongodb:latest
    container_name: mongodb-connector
    depends_on:
      - mongo_db
    environment:
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend


  worker_pubfinder: # --build for restart
    image: ghcr.io/ambalytics/amba-analysis-worker-pubfinder/amba-analysis-worker-pubfinder:latest
    container_name: pubfinder-worker
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 100M
    depends_on:
      - kafka
    environment:
      #      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      #      POSTGRES_USER: ${POSTGRES_USER}
      #      POSTGRES_DB: ${POSTGRES_DB}
      #      POSTGRES_HOST: ${POSTGRES_HOST}
      #      POSTGRES_PORT: ${POSTGRES_PORT}
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend

  aggregator:
    image: ghcr.io/ambalytics/amba-analysis-worker-aggregator/amba-analysis-worker-aggregator:latest
    container_name: aggregator
    depends_on:
      - kafka
    environment:
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend

  api:
    image: ghcr.io/ambalytics/amba-analysis-streams-api/amba-analysis-streams-api:latest
    container_name: api
    ports:
      - "8080:80"
    depends_on:
      - kafka
    environment:
      #      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      #      POSTGRES_USER: ${POSTGRES_USER}
      #      POSTGRES_DB: ${POSTGRES_DB}
      #      POSTGRES_HOST: ${POSTGRES_HOST}
      #      POSTGRES_PORT: 5432
      KAFKA_BOOTRSTRAP_SERVER: ${KAFKA_BOOTRSTRAP_SERVER}
    networks:
      - backend


  #  postgres:
  #    image: postgres
  ##    restart: always
  #    container_name: postgres
  #    environment:
  #      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #      POSTGRES_USER: ${POSTGRES_USER}
  #      POSTGRES_DB: ${POSTGRES_DB}
  #    ports:
  #      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
  #    networks:
  #      - backend
  #    volumes:
  #      - ./db:/docker-entrypoint-initdb.d/



  webserver:
    image: nginx:1.15.12-alpine
    container_name: webserver
    restart: unless-stopped
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./certbot/conf:/etc/nginx/ssl
      - ./certbot/data:/var/www/certbot
    ports:
      - "80:80"
      - "443:443"
    environment:
      - PHP_MEMORY_LIMIT=256M

  certbot:
    depends_on:
      - webserver
    image: certbot/dns-route53:latest
    container_name: certbot
    volumes:
      # we save our directory of keys on our host server
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/logs:/var/log/letsencrypt
    # command: certonly --dns-route53 --email it@ambalytics.com --agree-tos --no-eff-email -d ambalytics.com -d www.ambalytics.com -d ambalytics.cloud -d www.ambalytics.cloud -d 107.hosts.ambalytics.com
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

networks:
  backend:
    driver: bridge


# https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck
# for better waiting
#
# topicname:partition:replica
# "events_unlinked:1:1, events_unlinked-discussed:3:1, events_unlinked-crossref:3:1, events_linked:1:1, events_linked-discussed:3:1, events_unknown:3:1, events_processed:1:1, events_processed-discussed:3:1, events_aggregated:1:1"

# events_unlinked:1:1
# events_unlinked-discussed:3:1
# events_unlinked-crossref:3:1
# events_linked:1:1
# events_linked-discussed:3:1
# events_unknown:3:1
# events_processed:1:1
# events_processed-discussed:3:1
# events_aggregated:1:1

#  'unlinked': {
#            'own_topic': ['discussed', 'crossref']
#        },
#        'linked': {
#            'own_topic': ['discussed']
#        },
#        'unknown': {
#        },
#        'processed': {
#            'own_topic': ['discussed']
#        },
#        'aggregated': {

